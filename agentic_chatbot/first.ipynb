{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54be6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun ,WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7fe004b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import arxiv python package. Please install it with `pip install arxiv`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maviya Shaikh\\Desktop\\AGENTIC\\.venv\\Lib\\site-packages\\langchain_community\\utilities\\arxiv.py:81\u001b[39m, in \u001b[36mArxivAPIWrapper.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01marxiv\u001b[39;00m\n\u001b[32m     83\u001b[39m     values[\u001b[33m\"\u001b[39m\u001b[33marxiv_search\u001b[39m\u001b[33m\"\u001b[39m] = arxiv.Search\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'arxiv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m api_wrapper_arxiv=\u001b[43mArxivAPIWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_k_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdoc_content_chars_max\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv,description=\u001b[33m\"\u001b[39m\u001b[33muse this tool to search for research papers on arxiv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m arxiv.invoke(\u001b[33m\"\u001b[39m\u001b[33mWhat are the latest research papers on deep learning?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maviya Shaikh\\Desktop\\AGENTIC\\.venv\\Lib\\site-packages\\langchain_community\\utilities\\arxiv.py:91\u001b[39m, in \u001b[36mArxivAPIWrapper.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     89\u001b[39m     values[\u001b[33m\"\u001b[39m\u001b[33marxiv_result\u001b[39m\u001b[33m\"\u001b[39m] = arxiv.Result\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     92\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import arxiv python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install arxiv`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m     )\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mImportError\u001b[39m: Could not import arxiv python package. Please install it with `pip install arxiv`."
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv,description=\"use this tool to search for research papers on arxiv\")\n",
    "arxiv.invoke(\"What are the latest research papers on deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki,description=\"use this tool to search for information on wikipedia\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de10f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78dc264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maviya Shaikh\\AppData\\Local\\Temp\\ipykernel_20264\\2715124348.py:2: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tavily=TavilySearchResults()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Advancements in Deep Learning: A Comprehensive Study of the ...',\n",
       "  'url': 'https://www.academia.edu/144859758/Advancements_in_Deep_Learning_A_Comprehensive_Study_of_the_Latest_Trends_and_Techniques_in_Machine_Learning',\n",
       "  'content': 'This paper provides a comprehensive study of the latest trends and techniques in deep learning, a rapidly evolving field of machine learning. The paper begins by introducing the background of machine learning and the purpose of the study. Next, it provides an overview of deep learning, including its definition, history, key concepts, and techniques. The paper then examines the advancements in neural network architectures, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs). The paper also explores emerging applications of deep learning in computer vision, natural language processing, and reinforcement learning. The paper concludes by discussing the challenges and limitations of deep learning, including overfitting, [...] and limitations of deep learning, including overfitting, computational complexity, and explainability. Finally, the paper summarizes the advancements in deep learning, provides a perspective on future research directions, and highlights the implications for practice. This paper serves as a valuable resource for researchers, practitioners, and students interested in gaining a deeper understanding of the latest developments in deep learning. [...] multiple industrial modalities, we include an investigative section on testing neural networks for fault detection and subsequent mitigation. This is followed by an exploratory survey of several application areas where deep learning has emerged as a game-changing technology - be it anomalous behavior detection in financial applications or financial time-series forecasting, predictive and prescriptive analytics, medical imaging, natural language processing or power systems research. The thrust of this review is on outlining emerging areas of application-oriented research within the deep learning community as well as to provide a handy reference to researchers seeking to embrace deep learning in their work for what it is: statistical pattern recognizers with unparalleled hierarchical',\n",
       "  'score': 0.9997603},\n",
       " {'title': 'Review on recent trends and challenges in deep learning and ...',\n",
       "  'url': 'https://ui.adsabs.harvard.edu/abs/2022AIPC.2617b0010S/abstract',\n",
       "  'content': 'many developments in deep learning, there still exist some challenges. Many researchers are working on deep learning, but most of them focus on field-specific issues, while this paper presents a brief survey on deep-learning advancements from the last few decades. This work gives a clear and concise direction to new researchers and academicians. This article thoroughly discusses Deep Neural Network (DNN); it also covers different topics, i.e. Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including most of the other deep learning deep algorithms. This article also covers the open research issues and recent trends in deep learning. [...] Artificial Intelligence (AI) will shape human civilization differently because AI is increasingly playing a key role in our lives. AI is an Art that facilitate computer, robot or machine to mimic human behaviour. Deep learning is a subfield of AI that focuses more on how machine work and mimics the human brain. Deep learning is one of the recent, trending and emerging fields of study and research. Deep learning is revolutionizing every field from robotics to medicine and has provided an easy solution to many state-of-the-art problems. The recent developments in deep learning are virtual assistants, self-driving cars, news aggregation, natural language processing, digital marketing, visual recognition and image recognition etc. Despite many developments in deep learning, there still exist [...] Publication:\\n:   American Institute of Physics Conference Series\\n\\nPub Date:\\n:   November 2022\\n\\nDOI:\\n:   10.1063/5.0119678\\n\\nBibcode:\\n:   2022AIPC.2617b0010S\\n\\nfull text sources\\n\\nAIP\\n\\n|',\n",
       "  'score': 0.9985221},\n",
       " {'title': 'Famous Deep Learning Papers',\n",
       "  'url': 'https://papers.baulab.info/',\n",
       "  'content': 'Optimization (Rafailov, et al 2023) which proposes a much simpler training objective for fine-tuning LLMs. | [...] | 2025 | DeepSeek AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and over 100 additional auhors. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning | Can a neural net reason? By 2025 a vast amount of capital has poured into scaling up LLM training, with several companies competing to push benchmarks higher, and LLM training hits two barriers. First is the limit of imitation learning, after large-scale training has already incorporated most of the high-quality human-created data in the world. The second is the inability of a transformer LM to reason beyond the finite steps allowed by the fixed [...] | 2022 | Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser and Björn Ommer. High-Resolution Image Synthesis With Latent Diffusion Models | Can a neural net learn to draw? A set of 2022 papers mark a remarkable advance in the state-of-the-art in text-to-image synthesis. By applying diffusion models together with text supervision using CLIP representations, DALL-E 2 (Ramesh, et al. 2022) demonstrates an uncanny ability to create images from a text description that are obviously novel yet also realistic compositions of real ideas. Then by incorporating improved text conditioning from clasifier-free guidance (Ho and Salimans 2022), stacking diffusion onto an efficient VAE image representation, and then training the mdel on LAION (Schuhmann, et al. 2022), Latent Diffusion (the',\n",
       "  'score': 0.9984145},\n",
       " {'title': 'Latest papers - Journal of Machine Learning Research',\n",
       "  'url': 'https://jmlr.org/beta',\n",
       "  'content': 'Sungduk Yu, Zeyuan Hu, Akshay Subramaniam, Walter Hannah, Liran Peng, Jerry Lin, Mohamed Aziz Bhouri, Ritwik Gupta, Björn Lütjens, Justus C. Will, Gunnar Behrens, Julius J. M. Busecke, Nora Loose, Charles I Stern, Tom Beucler, Bryce Harrop, Helge Heuer, Benjamin R Hillman, Andrea Jenney, Nana Liu, Alistair White, Tian Zheng, Zhiming Kuang, Fiaz Ahmed, Elizabeth Barnes, Noah D. Brenowitz, Christopher Bretherton, Veronika Eyring, Savannah Ferretti, Nicholas Lutsko, Pierre Gentine, Stephan Mandt, J. David Neelin, Rose Yu, Laure Zanna, Nathan M. Urban, Janni Yuval, Ryan Abernathey, Pierre Baldi, Wayne Chuang, Yu Huang, Fernando Iglesias-Suarez, Sanket Jantre, Po-Lun Ma, Sara Shamekh, Guang Zhang, Michael Pritchard, 2025 [...] Didong Li, Andrew Jones, Sudipto Banerjee, Barbara E. Engelhardt, 2025\\n\\n#### Orthogonal Bases for Equivariant Graph Learning with Provable k-WL Expressive Power\\n\\nJia He, Maggie Cheng, 2025\\n\\n#### Optimal Experiment Design for Causal Effect Identification\\n\\nSina Akbari, Jalal Etesami, Negar Kiyavash, 2025\\n\\n#### Mean Aggregator is More Robust than Robust Aggregators under Label Poisoning Attacks on Distributed Heterogeneous Data\\n\\nJie Peng, Weiyu Li, Stefan Vlaski, Qing Ling, 2025\\n\\n#### The Blessing of Heterogeneity in Federated Q-Learning: Linear Speedup and Beyond\\n\\nJiin Woo, Gauri Joshi, Yuejie Chi, 2025\\n\\n#### depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning Researchers\\n\\nKaichao You, Runsheng Bai, Meng Cao, Jianmin Wang, Ion Stoica, Mingsheng Long, 2025 [...] Boxin Zhao, Lingxiao Wang, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Chaochao Chen, Mladen Kolar, 2025\\n\\n#### A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation\\n\\nHugo Lebeau, Florent Chatelain, Romain Couillet, 2025\\n\\n#### Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of Agents\\n\\nMarco Pleines, Matthias Pallasch, Frank Zimmer, Mike Preuss, 2025\\n\\n#### Enhancing Graph Representation Learning with Localized Topological Features\\n\\nZuoyu Yan, Qi Zhao, Ze Ye, Tengfei Ma, Liangcai Gao, Zhi Tang, Yusu Wang, Chao Chen, 2025\\n\\n#### Deep Out-of-Distribution Uncertainty Quantification via Weight Entropy Maximization\\n\\nAntoine de Mathelin, François Deheeger, Mathilde Mougeot, Nicolas Vayatis, 2025',\n",
       "  'score': 0.99596137},\n",
       " {'title': 'A Survey on State-of-the-art Deep Learning Applications and ...',\n",
       "  'url': 'https://arxiv.org/abs/2403.17561',\n",
       "  'content': \"Cornell University\\narxiv logo\\n\\nHelp | Advanced Search\\n\\narXiv logo\\nCornell University Logo\\n\\n## quick links\\n\\n# Computer Science > Machine Learning\\n\\n# Title:A Survey on State-of-the-art Deep Learning Applications and Challenges\\n\\n|  |  |\\n --- |\\n| Comments: | Update journal reference. This manuscript has been published in Engineering Applications of Artificial Intelligence (Elsevier) |\\n| Subjects: | Machine Learning (cs.LG) |\\n| Cite as: | arXiv:2403.17561 [cs.LG] |\\n|  | (or  arXiv:2403.17561v9 [cs.LG] for this version) |\\n|  |  Focus to learn more  arXiv-issued DOI via DataCite |\\n| Journal reference: | Engineering Applications of Artificial Intelligence, vol. 159, Nov 2025, p. 111225 |\\n| Related DOI: |   Focus to learn more  DOI(s) linking to related resources |\\n\\n## Submission history [...] arXiv Operational Status [...] ## Submission history\\n\\n## Access Paper:\\n\\n### References & Citations\\n\\n## BibTeX formatted citation\\n\\n### Bookmark\\n\\nBibSonomy logo\\nReddit logo\\n\\n# Bibliographic and Citation Tools\\n\\n# Code, Data and Media Associated with this Article\\n\\n# Demos\\n\\n# Recommenders and Search Tools\\n\\n# arXivLabs: experimental projects with community collaborators\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\n\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\",\n",
       "  'score': 0.9666631}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily=TavilySearchResults()\n",
    "tavily.invoke(\"What are the latest research papers on deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f12f62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arxiv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tools=[\u001b[43marxiv\u001b[49m,wiki,tavily]\n",
      "\u001b[31mNameError\u001b[39m: name 'arxiv' is not defined"
     ]
    }
   ],
   "source": [
    "tools=[arxiv,wiki,tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e9fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"qwen-2.5-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c24382",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools=llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c227dd6",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552a4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated ,  TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ccfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d10c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all graph libraries\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.prebuilt import ToolNode ,tools_condition\n",
    "from IPython.display import display ,Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0135168",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m builder=StateGraph(State)\n\u001b[32m      7\u001b[39m builder.add_node(\u001b[33m\"\u001b[39m\u001b[33mtool_calling\u001b[39m\u001b[33m\"\u001b[39m,tool_calling_llm)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m builder.add_node(\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m,ToolNode(tools=\u001b[43mtools\u001b[49m))\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#edges\u001b[39;00m\n\u001b[32m     12\u001b[39m builder.add_edge(START,\u001b[33m\"\u001b[39m\u001b[33mtool_calling\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tools' is not defined"
     ]
    }
   ],
   "source": [
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\": llm_with_tools.invoke(state[\"messages\"])}\n",
    "\n",
    "\n",
    "# building the graph\n",
    "builder=StateGraph(State)\n",
    "builder.add_node(\"tool_calling\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools=tools))\n",
    "\n",
    "\n",
    "#edges\n",
    "builder.add_edge(START,\"tool_calling\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling\",\n",
    "                              \n",
    "                 # if the llm calls a tool, then we go to the tools node\n",
    "                 # if the llm does not call a tool, then we end the graph execution             \n",
    "                              tools_condition,\n",
    "\n",
    ")\n",
    "builder.add_edge(\"tools\",\"tool_calling\")\n",
    "builder.add_edge(\"tool_calling\",END,condition=lambda state: not tools_condition(state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# react architecture \n",
    "buil\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AGENTIC (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
